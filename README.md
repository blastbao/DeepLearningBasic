# Deep Learning Basics
This is a DeepLearning basic tutorial. It's very suitable for beginners.<br />

[perceptron.py] (https://github.com/cxlsky/DeepLearningBasic/blob/master/perceptron.py) <br />
>Perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). <br />
>![image](https://github.com/cxlsky/image_folder/blob/master/Perceptron.png)<br />
>It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time.<br />

[linear_unit.py] (https://github.com/cxlsky/DeepLearningBasic/blob/master/linear_unit.py) <br />
>![image](https://github.com/cxlsky/image_folder/blob/master/linear_unit.png)<br />

[back_propagation.py] (https://github.com/cxlsky/DeepLearningBasic/blob/master/back_ropagation.py) <br />
>Backpropagation, an abbreviation for "backward propagation of errors", is a common method of training artificial neural networks used in conjunction with an optimization method such as gradient descent. It calculates the gradient of a loss function with respect to all the weights in the network, so that the gradient is fed to the optimization method which in turn uses it to update the weights, in an attempt to minimize the loss function.<br />
>![image](https://github.com/cxlsky/image_folder/blob/master/back_propagation.png)<br />
>Backpropagation requires a known, desired output for each input value in order to calculate the loss function gradient â€“ it is therefore usually considered to be a supervised learning method; nonetheless, it is also used in some unsupervised networks such as autoencoders. It is a generalization of the delta rule to multi-layered feedforward networks, made possible by using the chain rule to iteratively compute gradients for each layer. Backpropagation requires that the activation function used by the artificial neurons (or "nodes") be differentiable.<br />
